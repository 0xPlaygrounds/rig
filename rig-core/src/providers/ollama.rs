//! Ollama API client and Rig integration
//!
//! # Example
//! ```
//! use rig::providers::ollama;
//!
//! let client = ollama::Client::new();
//!
//! let llama = client.completion_model(ollama::LLAMA_3_2);
//! ```
use crate::{
    agent::AgentBuilder, embeddings::EmbeddingsBuilder, extractor::ExtractorBuilder,
    providers::openai::CompletionModel, Embed,
};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};

use super::openai::{self, EmbeddingModel};

// ================================================================
// Main Ollama Client
// ================================================================
const OLLAMA_API_BASE_URL: &str = "http://localhost:11434/v1";

#[derive(Clone)]
pub struct Client {
    client: openai::Client,
}

impl Client {
    /// Create a new Ollama client with the given API key.
    pub fn new() -> Self {
        Self::from_url("ollama", OLLAMA_API_BASE_URL)
    }

    /// Create a new Ollama client with the given API key and base API URL.
    pub fn from_url(api_key: &str, base_url: &str) -> Self {
        Self {
            client: openai::Client::from_url(api_key, base_url),
        }
    }

    /// Create a new Ollama client from the `OLLAMA_BASE_URL` environment variable.
    /// Panics if the environment variable is not set.
    pub fn from_env() -> Self {
        let api_key = std::env::var("OLLAMA_API_KEY").unwrap_or("ollama".to_string());
        let base_url = std::env::var("OLLAMA_BASE_URL").unwrap_or(OLLAMA_API_BASE_URL.to_string());
        Self::from_url(&api_key, &base_url)
    }

    /// Create an embedding model with the given name.
    /// Note: default embedding dimension of 0 will be used if model is not known.
    /// If this is the case, it's better to use function `embedding_model_with_ndims`
    ///
    /// # Example
    /// ```
    /// use rig::providers::ollama::{Client, self};
    ///
    /// // Initialize the Ollama client
    /// let ollama = Client::new();
    ///
    /// let embedding_model = ollama.embedding_model(ollama::LLAMA_3_2);
    /// ```
    pub fn embedding_model(&self, model: &str) -> EmbeddingModel {
        let ndims = match model {
            LLAMA_3_3 => 8192,
            LLAMA_3_2 => 3072,
            DEEPSEEK_R1 => 3584,
            PHI4 => 5120,
            MISTRAL => 4096,
            GEMMA2 => 3584,
            _ => 4096,
        };
        EmbeddingModel::new(self.client.clone(), model, ndims)
    }

    /// Create an embedding model with the given name and the number of dimensions in the embedding generated by the model.
    ///
    /// # Example
    /// ```
    /// use rig::providers::ollama::{Client, self};
    ///
    /// // Initialize the Ollama client
    /// let ollama = Client::new();
    ///
    /// let embedding_model = ollama.embedding_model_with_ndims("qwen2.5", 3584);
    /// ```
    pub fn embedding_model_with_ndims(&self, model: &str, ndims: usize) -> EmbeddingModel {
        EmbeddingModel::new(self.client.clone(), model, ndims)
    }

    /// Create an embedding builder with the given embedding model.
    ///
    /// # Example
    /// ```
    /// use rig::providers::ollama::{Client, self};
    ///
    /// // Initialize the Ollama client
    /// let ollama = Client::new();
    ///
    /// let embeddings = ollama.embeddings(ollama::LLAMA_3_2)
    ///     .documents(vec!["Hello, world!", "Goodbye, world!"])
    ///     .expect("Failed to embed documents")
    ///     .build();
    /// ```
    pub fn embeddings<D: Embed>(&self, model: &str) -> EmbeddingsBuilder<EmbeddingModel, D> {
        EmbeddingsBuilder::new(self.embedding_model(model))
    }

    /// Create a completion model with the given name.
    ///
    /// # Example
    /// ```
    /// use rig::providers::ollama::{Client, self};
    ///
    /// // Initialize the Ollama client
    /// let ollama = Client::new();
    ///
    /// let llama3_2 = ollama.completion_model(ollama::LLAMA_3_2);
    /// ```
    pub fn completion_model(&self, model: &str) -> CompletionModel {
        CompletionModel::new(self.client.clone(), model)
    }

    /// Create an agent builder with the given completion model.
    ///
    /// # Example
    /// ```
    /// use rig::providers::ollama::{Client, self};
    ///
    /// // Initialize the Ollama client
    /// let ollama = Client::new();
    ///
    /// let agent = ollama.agent(ollama::LLAMA_3_2)
    ///    .preamble("You are comedian AI with a mission to make people laugh.")
    ///    .temperature(0.0)
    ///    .build();
    /// ```
    pub fn agent(&self, model: &str) -> AgentBuilder<CompletionModel> {
        AgentBuilder::new(self.completion_model(model))
    }

    /// Create an extractor builder with the given completion model.
    /// The extractor builder is used to extract structured data from the completion response.
    pub fn extractor<T: JsonSchema + for<'a> Deserialize<'a> + Serialize + Send + Sync>(
        &self,
        model: &str,
    ) -> ExtractorBuilder<T, CompletionModel> {
        ExtractorBuilder::new(self.completion_model(model))
    }
}

/// ================================================================
/// Ollama Embedding Models
/// Refer to the [Ollama documentation](https://ollama.com/) for more information.
/// `llama3.3` embedding model
pub const LLAMA_3_3: &str = "llama3.3";
/// `llama3.2` embedding model
pub const LLAMA_3_2: &str = "llama3.2";
/// `deepseek-r1` embedding model
pub const DEEPSEEK_R1: &str = "deepseek-r1";
/// `phi4` embedding model
pub const PHI4: &str = "phi4";
/// `mistral` embedding model
pub const MISTRAL: &str = "mistral";
/// `gemma2` completion model
pub const GEMMA2: &str = "gemma2";

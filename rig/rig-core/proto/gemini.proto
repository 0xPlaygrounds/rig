// Minimal Gemini gRPC API proto for chat completion and streaming
// Extracted from google.ai.generativelanguage.v1beta
// Only includes types needed for GenerateContent and StreamGenerateContent RPCs

syntax = "proto3";

package gemini;

// GenerativeService provides AI text generation capabilities
service GenerativeService {
  // Generates a model response given an input GenerateContentRequest
  rpc GenerateContent(GenerateContentRequest) returns (GenerateContentResponse);
  
  // Generates a streamed response from the model given an input GenerateContentRequest
  rpc StreamGenerateContent(GenerateContentRequest) returns (stream GenerateContentResponse);
  
  // Generates embedding vectors from input content
  rpc EmbedContent(EmbedContentRequest) returns (EmbedContentResponse);
}

// Request to generate a completion from the model
message GenerateContentRequest {
  // Required. The name of the Model to use for generating the completion
  // Format: `models/{model}`
  string model = 1;
  
  // Required. The content of the current conversation with the model
  repeated Content contents = 2;
  
  // Optional. Developer set system instruction(s)
  optional Content system_instruction = 8;
  
  // Optional. A list of Tools the Model may use to generate the next response
  repeated Tool tools = 5;
  
  // Optional. Tool configuration for any Tool specified in the request
  optional ToolConfig tool_config = 7;
  
  // Optional. A list of unique SafetySetting instances for blocking unsafe content
  repeated SafetySetting safety_settings = 3;
  
  // Optional. Configuration options for model generation and outputs
  optional GenerationConfig generation_config = 4;
}

// Response from the model supporting multiple candidate responses
message GenerateContentResponse {
  // Response ID
  string response_id = 1;
  
  // Candidate responses from the model
  repeated ContentCandidate candidates = 2;
  
  // Returns the prompt's feedback related to the content filters
  optional PromptFeedback prompt_feedback = 3;
  
  // Metadata on the generation requests' token usage
  optional UsageMetadata usage_metadata = 4;
  
  // Model version
  optional string model_version = 5;
}

// The base unit of structured text
message Content {
  // Ordered Parts that constitute a single message
  repeated Part parts = 1;
  
  // The producer of the content. Must be either 'user' or 'model'
  optional string role = 2;
}

// A response candidate generated from the model
message ContentCandidate {
  // Generated content returned from the model
  optional Content content = 1;
  
  // The reason why the model stopped generating tokens
  optional string finish_reason = 2;
  
  // List of ratings for the safety of a response candidate
  repeated SafetyRating safety_ratings = 3;
  
  // Token count for this candidate
  optional int32 token_count = 5;
  
  // Average log probability
  optional double avg_logprobs = 6;
  
  // Index of the candidate
  optional int32 index = 8;
  
  // Additional information about why the model stopped generating tokens
  optional string finish_message = 9;
}

// A datatype containing media that is part of a multi-part Content message
message Part {
  // Whether or not the part is a reasoning/thinking text
  optional bool thought = 1;
  
  // Signature for the thought so it can be reused
  optional string thought_signature = 2;
  
  // The actual content (one of the following)
  oneof data {
    string text = 3;
    Blob inline_data = 4;
    FunctionCall function_call = 5;
    FunctionResponse function_response = 6;
    FileData file_data = 7;
    ExecutableCode executable_code = 8;
    CodeExecutionResult code_execution_result = 9;
  }
}

// Raw media bytes with associated MIME type
message Blob {
  string mime_type = 1;
  string data = 2;  // base64 encoded
}

// URI based data
message FileData {
  optional string mime_type = 1;
  string file_uri = 2;
}

// A predicted FunctionCall returned from the model
message FunctionCall {
  string name = 1;
  optional string args = 2;  // JSON string
}

// The result output from a FunctionCall
message FunctionResponse {
  string name = 1;
  optional string response = 2;  // JSON string
}

// Code generated by the model to be executed
message ExecutableCode {
  string language = 1;  // e.g., "PYTHON" 
  string code = 2;
}

// Result of code execution
message CodeExecutionResult {
  string outcome = 1;  // e.g., "OUTCOME_OK", "OUTCOME_FAILED"
  optional string output = 2;
}

// Tool details that the model may use to generate response
message Tool {
  repeated FunctionDeclaration function_declarations = 1;
  optional CodeExecution code_execution = 2;
}

// Structured representation of a function declaration
message FunctionDeclaration {
  string name = 1;
  string description = 2;
  optional Schema parameters = 3;
}

// Code execution tool
message CodeExecution {}

// The Schema object for function parameters (simplified OpenAPI schema)
message Schema {
  optional string type = 1;
  optional string format = 2;
  optional string description = 3;
  optional bool nullable = 4;
  repeated string enum = 5;
  map<string, Schema> properties = 6;
  repeated string required = 7;
  optional Schema items = 8;
}

// Tool configuration
message ToolConfig {
  optional FunctionCallingConfig function_calling_config = 1;
}

// Configuration for function calling
message FunctionCallingConfig {
  optional string mode = 1;  // "AUTO", "ANY", "NONE"
  repeated string allowed_function_names = 2;
}

// Safety setting for a single category
message SafetySetting {
  string category = 1;  // e.g., "HARM_CATEGORY_HATE_SPEECH"
  string threshold = 2;  // e.g., "BLOCK_MEDIUM_AND_ABOVE"
}

// Safety rating for a piece of content
message SafetyRating {
  string category = 1;
  string probability = 2;
  optional bool blocked = 3;
}

// Feedback related to the content filters
message PromptFeedback {
  optional string block_reason = 1;
  repeated SafetyRating safety_ratings = 2;
}

// Metadata on token usage
message UsageMetadata {
  int32 prompt_token_count = 1;
  optional int32 candidates_token_count = 2;
  int32 total_token_count = 3;
  optional int32 cached_content_token_count = 4;
}

// Configuration options for model generation
message GenerationConfig {
  optional int32 candidate_count = 1;
  repeated string stop_sequences = 2;
  optional int32 max_output_tokens = 4;
  optional float temperature = 5;
  optional float top_p = 6;
  optional int32 top_k = 7;
  optional int32 seed = 8;
  optional string response_mime_type = 13;
  optional Schema response_schema = 14;
  optional ThinkingConfig thinking_config = 15;
}

// Configuration for thinking/reasoning features
message ThinkingConfig {
  optional bool include_thoughts = 1;
  optional int32 thinking_budget = 2;
}

// Request for embedding generation
message EmbedContentRequest {
  // Required. The model to use
  string model = 1;
  
  // Required. The content to embed
  Content content = 2;
  
  // Optional. The task type
  optional string task_type = 3;
  
  // Optional. The title for the content
  optional string title = 4;
  
  // Optional. Reduced output dimensionality
  optional int32 output_dimensionality = 5;
}

// Response for embedding generation
message EmbedContentResponse {
  // The embedding values
  ContentEmbedding embedding = 1;
}

// A list of floats representing an embedding
message ContentEmbedding {
  repeated float values = 1;
}
